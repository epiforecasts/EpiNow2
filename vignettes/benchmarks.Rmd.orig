---
title: "Model benchmarks: speed versus forecast accuracy tradeoffs"
output:
  rmarkdown::html_vignette:
    toc: true
    code_folding: show
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Model benchmarks: speed versus forecast accuracy tradeoffs}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  fig.height = 6.5,
  fig.width = 6.5,
  fig.path = "vignettes/speedup_options-"
)
set.seed(9876)
```

```{r packages}
library(EpiNow2)
library(scoringutils)
library(data.table)
library(rstan)
library(cmdstanr)
library(ggplot2)
library(dplyr)
library(lubridate)
library(scales)
library(posterior)
```

In using `{EpiNow2}`, users will often be faced with two decision points that will guide their choice of an appropriate model: (i) use case: retrospective vs real-time analysis, and (ii) limited computing resources. `{EpiNow2}` provides a range of customisations of the default model to suit these decision points.

The aim of this vignette is to show how these model customisations affect the speed and accuracy of the model. We will benchmark four (4) `{EpiNow2}` model options chosen to cover a range of use cases. We will benchmark the default model, the model with the 7-day random walk prior on $R_t$, the non-mechanistic model, which has no explicit prior on $R_t$, and the non-residual $R_t$ model, which assumes a stationary prior on $R_t$.

We will analyse the estimation and forecasting performance of these models when solved with MCMC sampling. `{EpiNow2}` also provides approximate sampling methods to solve these models ([variational inference](https://mc-stan.org/docs/cmdstan-guide/variational_config.html), [pathfinder method](https://mc-stan.org/docs/cmdstan-guide/pathfinder_config.html), and [laplace sampling](https://mc-stan.org/docs/cmdstan-guide/laplace_sample_config.html)). These algorithms are, however, not recommended for use in pipelines, so we will not emphasise their use. We will, however, highlight how the models perform when solved with these approximate sampling methods and provide an overview of when they may be appropriate.

For the benchmarking exercise, we will compare the models based on their runtimes and the accuracy of estimates on complete and partial data, taken at 3 time points of a simulated epidemic: (i) the growth phase, (ii) the peak, and (iii) the decline phase.

## The benchmarking data

We will start by setting up the "true" dataset with known trajectories of $R_t$ and infections using `{EpiNow2}`'s `forecast_infections()` function.

`forecast_infections()` requires a fitted estimates object from `epinow()` with `output` set to "fit", the trajectory of the reproduction number, `R`, and the number of samples to simulate. So, we will set these up first.

To obtain the `estimates` object, we will run the `epinow()` function using real-world observed data and delay distributions to recover realistic parameter values. For the `data`, we will use the first $60$ observations of the `example_confirmed` data set. We will use the `example_generation_time` for the generation time, and the `example_incubation_period`, and `example_reporting_delay` to specify  and delays. These come with the package. For the `rt` model, we will use a 14-day random walk prior, with a mean of $2$ and standard deviation of $0.1$. As we only want to generate estimates, we will turn off forecasting by setting `horizon = 0`.

Throughout this vignette, several argument values, including the observation model options, the rt model prior will be maintained, so we will define them here.

```{r share_inputs}
# Observation model options
obs <- obs_opts(
  scale = Normal(0.1, 0.025),
  return_likelihood = TRUE
)
# Rt prior
rt_prior_default <- Normal(2, 0.1)
# Number of cores
options(mc.cores = 6)
```

Now, we can generate the `estimates` object.
```{r estimates}
estimates <- epinow(
  data = example_confirmed[1:60],
  generation_time = generation_time_opts(example_generation_time),
  delays = delay_opts(example_incubation_period + example_reporting_delay),
  rt = rt_opts(prior = rt_prior_default, rw = 14),
  gp = NULL,
  obs = obs,
  forecast = forecast_opts(horizon = 0), # no forecasting
  output = "fit"
)
```

For the `R` data, we will set up an arbitrary trajectory and add some Gaussian noise.
```{r R-data}
# Arbitrary reproduction number trajectory
R <- c(
  rep(1.50, 20),
  rep(1.25, 10),
  rep(1, 10),
  rep(0.5, 10),
  rep(1, 10),
  1 + 0.04 * 1:20,
  rep(1.4, 5),
  1.4 - 0.02 * 1:20,
  rep(1.4, 10),
  rep(0.8, 50),
  0.8 + 0.02 * 1:20
)
# Add Gaussian noise
R_noisy <- R * rnorm(length(R), 1.1, 0.05)
# Plot
ggplot(data = data.frame(R = R_noisy)) +
  geom_line(aes(x = seq_along(R_noisy), y = R)) +
  labs(x = "Time")
```

Let's proceed to simulate the true infections and $R_t$ data by sampling from $10$ posterior samples.
```{r true-data}
# Forecast infections and the trajectory of Rt
forecast <- forecast_infections(
  estimates$estimates,
  R = R_noisy,
  samples = 10
)
```

Let's now extract the true data for benchmarking:
- `R_true`: the median of the simulated $R_t$ values,
- `infections_true`: the infections by date of infection, and
- `reported_cases_true`: the reported cases by date of report.
```{r extract-true-data}
R_true <- forecast$summarised[variable == "R"]$median

# Get the posterior samples from which to extract the simulated infections and reported cases
posterior_sample <- forecast$samples[sample == 1]

# Extract the simulated infections
infections_true <- posterior_sample[variable == "infections"]$value

# Extract the simulated reported cases and rename the "value" column to "confirm" (to match EpiNow2 requirements)
reported_cases_true <- posterior_sample[
  variable == "reported_cases", .(date, confirm = value)
]
```

Let's see what the cases plot looks like
```{r plot-cases}
cases_traj <- ggplot(data = reported_cases_true) +
  geom_line(aes(x = date, y = confirm)) +
  scale_y_continuous(label = scales::label_comma()) +
  scale_x_date(date_labels = "%b %d", date_breaks = "1 month") +
  labs(y = "Reported cases", x = "Date")
cases_traj
```


We will now proceed to define and run the different model options.

## Models

### Descriptions

Below we describe each model.
```{r model-descriptions,echo = FALSE}
model_descriptions <- dplyr::tribble(
  ~model,  ~model_basename, ~description,
  "default_mcmc",   "default",   "Default model (non-stationary prior on $R_t$); fitting with mcmc",
  "default_vb",  "default", "Default model (non-stationary prior on $R_t$); fitting with variational bayes",
  "default_pathfinder",  "default", "Default model (non-stationary prior on $R_t$); fitting with pathfinder algorithm",
  "default_laplace",  "default", "Default model (non-stationary prior on $R_t$); fitting with laplace approximation",
  "non_mechanistic_mcmc", "non_mechanistic", "no mechanistic prior on $R_t$; fitting with mcmc",
  "non_mechanistic_vb", "non_mechanistic", "no mechanistic prior on $R_t$; fitting with variational bayes",
  "non_mechanistic_pathfinder", "non_mechanistic", "no mechanistic prior on $R_t$; fitting with pathfinder algorithm",
  "non_mechanistic_laplace",  "non_mechanistic", "no mechanistic prior on $R_t$; fitting with laplace approximation",
  "rw7_mcmc", "rw7", "7-day random walk prior on $R_t$; fitting with mcmc",
  "rw7_vb", "rw7", "7-day random walk prior on $R_t$; fitting with variational bayes",
  "rw7_pathfinder", "rw7", "7-day random walk prior on $R_t$; fitting with pathfinder algorithm",
  "rw7_laplace",  "rw7", "7-day random walk prior on $R_t$; fitting with laplace approximation",
  "non_residual_mcmc",  "non_residual",  "Stationary prior on $R_t$; fitting with mcmc",
  "non_residual_vb",  "non_residual", "Stationary prior on $R_t$; fitting with variational bayes",
  "non_residual_pathfinder",  "non_residual",  "Stationary prior on $R_t$; fitting with pathfinder algorithm",
  "non_residual_laplace",   "non_residual", "Stationary prior on $R_t$; fitting with laplace algorithm"
)
  
knitr::kable(model_descriptions, caption = "Model options")
```

These are the components of each model.
```{r model-components,echo = FALSE}
model_components <- dplyr::tribble(
  ~model,                              ~rt_gp_prior,                       ~fitting,                 ~package,
  "default_mcmc",                        "non_stationary",                  "mcmc",                     "rstan", 
  "default_vb",                           "non_stationary",                 "variational_bayes",         "rstan", 
  "default_pathfinder",                    "non_stationary",                "pathfinder",                 "cmdstanr",
  "default_laplace",                        "non_stationary",               "laplace",                    "cmdstanr",
  "non_mechanistic_mcmc",                     "none",                       "mcmc",                        "rstan",
  "non_mechanistic_vb",                       "none",                       "variational_bayes",            "rstan",
  "non_mechanistic_pathfinder",               "none",                       "pathfinder",                   "cmdstanr",
  "non_mechanistic_laplace",                   "none",                      "laplace",                      "cmdstanr",
  "rw7_mcmc",                                  "non_stationary",            "mcmc",                          "rstan",
  "rw7_vb",                                     "non_stationary",            "variational_bayes",             "rstan",
  "rw7_pathfinder",                              "non_stationary",            "pathfinder",                   "cmdstanr",
  "rw7_laplace",                                  "non_stationary",            "laplace",                     "cmdstanr",
  "non_residual_mcmc",                             "stationary",               "mcmc",                        "rstan",
  "non_residual_vb",                                 "stationary",            "variational_bayes",             "rstan",
  "non_residual_pathfinder",                           "stationary",            "pathfinder",                  "rstan",
  "non_residual_laplace",                               "stationary",            "laplace",                     "cmdstanr"
  )
  
knitr::kable(model_components, caption = "Model components")
```

### Configurations

We will now define the `{EpiNow2}` configurations for each model, which are modifications of the default model.
```{r model-configs, results = 'hide'}
model_configs <- list(
  # The default model with MCMC fitting
  default_mcmc = list(
    rt = rt_opts()
  ),
  # The default model with variational bayes fitting
  default_vb = list(
    rt = rt_opts(),
    stan = stan_opts(method = "vb", backend = "rstan")
  ),
  # The default model with pathfinder fitting
  default_pathfinder = list(
    rt = rt_opts(),
    stan = stan_opts(method = "pathfinder", backend = "cmdstanr")
  ),
  # The default model with laplace fitting
  default_laplace = list(
    rt = rt_opts(),
    stan = stan_opts(method = "laplace", backend = "cmdstanr")
  ),
  # The non-mechanistic model with MCMC fitting
  non_mechanistic_mcmc = list(
    rt = NULL
  ),
  # The non-mechanistic model with variational bayes fitting
  non_mechanistic_vb = list(
    rt = NULL,
    stan = stan_opts(method = "vb", backend = "rstan")
  ),
  # The non-mechanistic model with pathfinder fitting
  non_mechanistic_pathfinder = list(
    rt = NULL,
    stan = stan_opts(method = "pathfinder", backend = "cmdstanr")
  ),
  # The non-mechanistic model with laplace fitting
  non_mechanistic_laplace = list(
    rt = NULL,
    stan = stan_opts(method = "laplace", backend = "cmdstanr")
  ),
  # The 7-day RW Rt model with MCMC fitting
  rw7_mcmc = list(
    rt = rt_opts(
      prior = rt_prior_default,
      rw = 7
    )
  ),
  # The 7-day RW Rt model with variational bayes fitting
  rw7_vb = list(
    rt = rt_opts(
      prior = rt_prior_default,
      rw = 7
    ),
    stan = stan_opts(method = "vb", backend = "rstan")
  ),
  # The 7-day RW Rt model with pathfinder fitting
  rw7_pathfinder = list(
    rt = rt_opts(
      prior = rt_prior_default,
      rw = 7
    ),
    stan = stan_opts(method = "pathfinder", backend = "cmdstanr")
  ),
  # The 7-day RW Rt model with laplace fitting
  rw7_laplace = list(
    rt = rt_opts(
      prior = rt_prior_default,
      rw = 7
    ),
    stan = stan_opts(method = "laplace", backend = "cmdstanr")
  ),
  # The non_residual model with MCMC fitting
  non_residual_mcmc = list(
    rt = rt_opts(
      prior = rt_prior_default,
      gp_on = "R0"
    )
  ),
  # The non_residual model with variational bayes fitting
  non_residual_vb = list(
    rt = rt_opts(
      prior = rt_prior_default,
      gp_on = "R0"
    ),
    stan = stan_opts(method = "vb", backend = "rstan")
  ),
  # The non_residual model with pathfinder fitting
  non_residual_pathfinder = list(
    rt = rt_opts(
      prior = rt_prior_default,
      gp_on = "R0"
    ),
    stan = stan_opts(method = "pathfinder", backend = "cmdstanr")
  ),
  # The non_residual model with laplace fitting
  non_residual_laplace = list(
    rt = rt_opts(
      prior = rt_prior_default,
      gp_on = "R0"
    ),
    stan = stan_opts(method = "pathfinder", backend = "cmdstanr")
  )
)
```

## Running the models

All the models will share the configuration for the generation time, incubation period, reporting delay, and the forecast horizon, so we will define them once and pass them to the models.

```{r, constant_inputs}
# Generation time
generation_time <- Gamma(
  shape = Normal(1.3, 0.3),
  rate = Normal(0.37, 0.09),
  max = 14
)

# Incubation period
incubation_period <- LogNormal(
  meanlog = Normal(1.6, 0.05),
  sdlog = Normal(0.5, 0.05),
  max = 14
)

# Reporting delay
reporting_delay <- LogNormal(
  meanlog = 0.5,
  sdlog = 0.5,
  max = 10
)

# Combine the incubation period and reporting delay into one delay
delay <- incubation_period + reporting_delay

# 7-day forecast window
horizon <- 7

# Combine the shared model inputs into a list for use across all the models
model_inputs <- list(
  generation_time = generation_time_opts(generation_time),
  delays = delay_opts(delay),
  obs = obs,
  forecast = forecast_opts(horizon = horizon),
  verbose = FALSE
)
```

Additionally, from the benchmarking data, we choose the following dates to represent the periods of growth, peak, and decline.
```{r snapshot_dates}
snapshot_dates <- as.Date(c("2020-05-15", "2020-06-21", "2020-06-28"))
```

Using the chosen dates, let's create the data snapshots for fitting the models.
```{r data-snapshots}
data_snaps <- lapply(
  snapshot_dates,
  function(snap_date) {
    reported_cases_true[date <= snap_date]
  }
)
names(data_snaps) <- snapshot_dates
```

Now, we're ready to run the models. We will use the `epinow()` function and return useful outputs like the timing of model runs. We obtain forecasts for the data excluding the forecast horizon and then compare the forecasts to the data including the horizon in the evaluations. This is often called an out-of-sample evaluation.
```{r run-models, results = 'hide'}
# Create a version of epinow() that works like base::try() and works even if some models fail.
safe_epinow <- purrr::safely(epinow)
# Run the models over the different dates
results <- lapply(
  data_snaps, function(data) {
    lapply(
      model_configs,
      function(model) {
        # Use subset of the data
        data_to_fit <- data[1:(.N - model_inputs$forecast$horizon)] # exclude the forecast horizon
        model_inputs <- c(model_inputs, data = list(data_to_fit))
        do.call(
          safe_epinow,
          c(
            model_inputs,
            model
          )
        )
      }
    )
  }
)
```

## Evaluating model performance

To start with, let's set up a function, `extract_results()` that extracts the "timing", "Rt", and "infections" variables from the `epinow()` runs for a supplied model.
```{r extraction-funcs, class.source = 'fold-hide'}
# Function to extract the "timing", "Rt", "infections", and "reports" variables from an 
# epinow() run. It expects a model run, x, which contains a "results" or "error" component.
# If all went well, "error" should be NULL.
extract_results <- function(x, variable) {
  stopifnot(
    "variable must be one of c(\"timing\", \"R\", \"infections\", \"reports\")" =
      variable %in% c("timing", "R", "infections", "reports")
  )
  # Return NA if there's an error
  if (!is.null(x$error)) {
    return(NA)
  }
  
  if(variable == "timing") {
    return(round(as.duration(x$result$timing), 1))
  } else {
    obj <- x$result$estimates$fit 
  }
  
  # Extracting "Rt", "infections", and "reports" is different based on the object's class and 
  # other settings
  if (inherits(obj, "stanfit")) {
    # Depending on rt_opts(use_rt = TRUE/FALSE), R shows up as R or gen_R
    if (variable == "R") {
      # The non-mechanistic model returns "gen_R" where as the others sample "R".
      if ("R[1]" %in% names(obj)) {
        return(extract(obj, "R")$R)
      } else {
        return(extract(obj, "gen_R")$gen_R)
      }
    } else {
      return(extract(obj, variable)[[variable]])
    }
  } else {
    obj_mat <- as_draws_matrix(obj)
    # Extracting R depends on the value of rt_opts(use_rt = )
    if (variable == "R") {
      if ("R[1]" %in% variables(obj_mat)) {
          return(subset_draws(obj_mat, "R"))
      } else {
        return(subset_draws(obj_mat, "gen_R"))
      }
    } else {
        return(subset_draws(obj_mat, variable))
      }
    }
}

# Wrapper for extracting the results and making them into a data.table
get_model_results <- function(results_by_snapshot, variable) {
  # Get model results list
  lapply(
    results_by_snapshot,
    function(model_results) {
      lapply(model_results, extract_results, variable)
    }
  )
}
```


### Run times (computational resources)

Let's see how long each model took to run.
```{r runtimes}
# Extract the run times and reshape to dt
runtimes_by_snapshot <- get_model_results(results, "timing")

runtimes_dt <- lapply(runtimes_by_snapshot, function(x) as.data.table(x)) |>
  rbindlist(idcol = "snapshot_date", ignore.attr = TRUE)

# Reshape
runtimes_dt_long <- melt(
  runtimes_dt,
  id.vars = "snapshot_date",    # Column to keep as an identifier
  measure.vars = model_descriptions$model,  # Dynamically select model columns by pattern
  variable.name = "model",      # Name for the 'model' column
  value.name = "timing"         # Name for the 'timing' column
)
  
# Add model configurations
runtimes_dt_detailed <- merge(
  runtimes_dt_long,
  model_components,
  by = "model"
)

# snapshot dates dictionary
snapshot_date_names <- c(growth = "2020-05-15", peak = "2020-06-21", decline = "2020-06-28")

# Replace snapshot_date based on the dictionary
runtimes_dt_detailed[, epidemic_phase := names(snapshot_date_names)[match(snapshot_date, snapshot_date_names)]]

# Move some columns around
setcolorder(runtimes_dt_detailed, "timing", after = "package")

# Make all columns except timing a factor
runtimes_dt_detailed[
  ,
  (setdiff(names(runtimes_dt_detailed), "timing")) :=
    lapply(.SD, as.factor),
  .SDcols = setdiff(names(runtimes_dt_detailed), "timing")
]

# Add model descriptions
runtimes_dt_detailed <- merge(
  runtimes_dt_detailed,
  model_descriptions,
  by = "model"
)

# Add fit type
runtimes_dt_detailed[, fit_type := ifelse(fitting == "mcmc", "mcmc", "approximate")]

# Add epidemic phase as a factor
runtimes_dt_detailed[, epidemic_phase := factor(epidemic_phase, levels = c("growth", "peak", "decline"))]

# Plot the timing
timing_plot <- ggplot(data = runtimes_dt_detailed) +
  geom_point(aes(x = epidemic_phase,
               y = timing,
               color = model_basename,
               shape = rt_gp_prior
               ),
             size = 2.2
  ) +
  labs(x = "Epidemic phase",
       y = "Runtime (secs)",
       shape = "Rt model prior",
       color = "Model",
       caption = "non-stationary Rt = R(t-1) * GP; stationary Rt = R0 * GP; non-mechanistic Rt = no GP prior."
       ) +
  theme_minimal() +
  facet_wrap(~fit_type, scales = "free_y", nrow = 2, strip.position = "left")
timing_plot
```

### Evaluating model performance

Now, we will evaluate the performance of the models using the [continuous ranked probability score (CRPS)](https://en.wikipedia.org/wiki/Scoring_rule#Continuous_ranked_probability_score). The CRPS is a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule#Propriety_and_consistency) that measures the accuracy of probabilistic forecasts. The smaller the CRPS, the better. We will use the [`crps_sample()`](https://epiforecasts.io/scoringutils/reference/crps_sample.html) function from the [`{scoringutils}`](https://epiforecasts.io/scoringutils/index.html) package because it matches the type of forecast data returned by `epinow()`.

Recall that we are only evaluating the models solved with MCMC sampling. We will not evaluate the approximate sampling methods as they are not recommended for use in analytics pipelines and for inference.

To calculate the CRPS for the estimated $R_t$ and infections, we will first set up a function that ensures the true data and estimates are of the same length and then calls the `crps_sample()` function from the `{scoringutils}` package using log-transformed values of the true and estimated values.
```{r crps-func}
# A function to calculate the CRPS
calc_crps <- function(estimates, truth) {
  # if the object is not a matrix, then it's an NA (failed run)
  if (!inherits(estimates, c("matrix"))) return(rep(NA_real_, length(truth)))
  # Assumes that the estimates object is structured with the samples as rows
  shortest_obs_length <- min(ncol(estimates), length(truth))
  reduced_truth <- tail(truth, shortest_obs_length)
  estimates_transposed <- t(estimates) # transpose to have samples as columns
  reduced_estimates <- tail(estimates_transposed, shortest_obs_length)
  return(
    crps_sample(
      log10(reduced_truth),
      log10(reduced_estimates)
    )
  )
}
```

We will set up a function that processes the CRPS calculations and postprocesses the results by adding dates and model configurations and descriptions for easier visualisation downstream.
```{r crps-processing, class.source = 'fold-hide'}
# Define a function to process CRPS calculations and data transformation
process_crps <- function(results, variable, truth) {
  # Extract values
  by_snapshot <- get_model_results(results, variable = variable)

  # Apply function to calculate CRPS
  crps_by_snapshot <- lapply(
    by_snapshot,
    function(snapshot_results) {
      lapply(
        snapshot_results, function(model_results) {
          calc_crps(estimates = model_results, truth = truth)
        }
      )
    }
  )

  # Add dates column based on snapshot length
  crps_with_dates <- lapply(
    crps_by_snapshot,
    function(snapshot_results) {
      lapply(
        snapshot_results,
        function (model_results) {
          data.table(crps = model_results)[,
            date:= min(reported_cases_true$date) + 0: (.N - 1)
          ]
        }
      )
    })

  # Flatten the results into one dt
  crps_flat <- lapply(
    crps_with_dates,
    function(snapshot_results) {
     rbindlist(snapshot_results, idcol = "model")
    }) |>
    rbindlist(idcol = "snapshot_date")

  # Add model configurations for facetting
  crps_full <- merge.data.table(
    crps_flat,
    model_components,
    by = "model"
  )

  # Add model descriptions
  crps_full <- merge.data.table(
    crps_full,
    model_descriptions,
    by = "model"
  )

  # Replace the snapshot dates with their description
  crps_full[, epidemic_phase := names(snapshot_date_names)[
    match(snapshot_date, snapshot_date_names)
  ]]

  return(crps_full)
}
```

Now, let's apply the `process_crps()` function to extract the $R_t$ and infections estimates and calculate the CRPS using the `calc_crps()` function above.
```{r process-crps}
# Process CRPS for Rt
rt_crps_full <- process_crps(results, "R", R)

# Process CRPS for infections
infections_crps_full <- process_crps(results, "infections", infections_true)
```

In order to differentiate between the subsets of the estimates, we will add the "type" column from the output of the model to indicate which subsets of the estiamtes are based on partial and complete data. We will get the "type" column from the default model (the same across model outputs) and add it to the CRPS results.
```{r estimates-types-by-date}
# Get date and fit type from the default model (the same across model outputs)
results_by_model <- list_transpose(results)

fit_type_by_dates <- lapply(
  results_by_model$default_mcmc,
  function(results_by_snapshot) {
    results_by_snapshot$result$estimates$summarised[
      variable == "reported_cases"][
        ,
        c("date", "type")
      ]
  }
) |>
  rbindlist(idcol = "snapshot_date")
```

Now we will add the "type" column to the CRPS results.
```{r crps-postprocessing}
# # Add the "type" column
rt_crps_dt_final <- merge(
  rt_crps_full,
  fit_type_by_dates,
  by = c("date", "snapshot_date")
)

# Add the "type" column
infections_crps_dt_final <- merge(
  infections_crps_full,
  fit_type_by_dates,
  by = c("date", "snapshot_date")
)
```

#### Model performance over time

We will now set up a function to plot the CRPS over time.
```{r crps-plotting-functions, class.source = 'fold-hide'}
# Define a function to plot CRPS over time
plot_crps_over_time <- function(data, title) {
  ggplot(data = data[!is.na(crps)], # remove failed models
  ) +
    geom_line(
      aes(x = date,
          y = crps,
          linetype = rt_gp_prior,
          color = model
          )
    ) +
    scale_linetype_manual(values = c(1, 5, 3)) +
    labs(
      x = "Time",
      y = "CRPS",
      title = title
    ) +
    ggplot2::theme_minimal() +
    guides(color = guide_legend(title = "Model type")) +
    theme(legend.position = "bottom") +
    facet_grid(~epidemic_phase)
}

```

We will now plot the CRPS over time for the $R_t$ estimates.
```{r crps-plotting-rt}
# Plot CRPS over time for Rt
rt_crps_mcmc <- plot_crps_over_time(rt_crps_dt_final[fit_type == "mcmc"], "Estimating Rt")
plot(rt_crps_mcmc)
```

Let's do the same for the infections estimates.
```{r crps-plotting-infections}
# Plot CRPS over time for infections
infections_crps_mcmc <- plot_crps_over_time(infections_crps_dt_final, "Estimating and predicting infections")
plot(infections_crps_mcmc)
```

#### Overall model performance

We will show the overall performance of the models by calculating and plotting the total CRPS.
```{r crps-total-calc}
# Define a function to calculate total CRPS
calculate_total_crps <- function(data) {
  data[
    ,
    .(
      total_crps = sum(crps),
      rt_gp_prior = rt_gp_prior[1],
      fitting = fitting[1],
      package = package[1]
    ),
    by = .(model, epidemic_phase, type)
  ][
    type != "estimate"
  ]
}

# Define a function to plot total CRPS
plot_total_crps <- function(data, title) {
  ggplot(data = data) +
    geom_col(
      aes(
        x = type,
        y = total_crps,
        fill = rt_gp_prior
      ),
      position = position_dodge()
    ) +
    scale_fill_brewer(palette = "Dark2") +
    labs(
      x = "Estimate type",
      y = "Total CRPS",
      fill = "Model type",
      title = title
    ) +
    facet_grid(~epidemic_phase)
}
```

Let's show the total CRPS for the $R_t$ estimates.
```{r crps-plotting-rt-total}
# Calculate and plot total CRPS for Rt
rt_total_crps_mcmc <- calculate_total_crps(rt_crps_dt_final[fit_type == "mcmc"])
rt_total_crps_mcmc_plot <- plot_total_crps(rt_total_crps_mcmc, "Estimating Rt")
plot(rt_total_crps_mcmc_plot)
```

The total CRPS for the infections estimates is shown below as well.
```{r crps-plotting-infections-total}
# Calculate and plot total CRPS for infections
infections_total_crps_dt <- calculate_total_crps(infections_crps_dt_final)
infections_total_crps_plot <- plot_total_crps(infections_total_crps_dt, "Estimating infections")
plot(infections_total_crps_plot)
```

#### Performance of approximate methods

We will briefly look at the performance of the approximate methods although we do not recommend using them in real-world inference and analytics pipelines.

```{r crps-plotting-rt-approx}
# Plot CRPS over time for Rt with approximate methods
rt_crps_approx <- plot_crps_over_time(rt_crps_dt_final[fit_type != "mcmc"], "Estimating Rt with Approximate Methods")
plot(rt_crps_approx)
```

The total CRPS for the approximate methods is shown below.
```{r crps-plotting-rt-total-approx}
# Calculate and plot total CRPS for Rt with approximate methods
rt_total_crps_approx <- calculate_total_crps(rt_crps_dt_final[fit_type != "mcmc"])
rt_total_crps_approx_plot <- plot_total_crps(rt_total_crps_approx, "Estimating Rt with Approximate Methods")
plot(rt_total_crps_approx_plot)
```

From the results of the model run times and CRPS measures, we can see that no single model is the best for all tasks. There is often a trade-off between run times/speed and estimation/forecasting performance, here measured with the CRPS These results show that choosing an appropriate model for a task requires carefully considering the use case and appropriate trade-offs. Below are a few considerations.

## Things to consider when interpreting these benchmarks

### Mechanistic vs non-mechanistic models

Estimation in `{EpiNow2}` using the mechanistic approaches (prior on $R_t$) is often much slower than the non-mechanistic approach. The mechanistic model is slower because it models aspects of the processes and mechanisms that drive $R_t$ estimates using the renewal equation. The non-mechanistic model, on the other hand, runs much faster but does not use the renewal equation to generate infections. Because of this none of the options defining the behaviour of the reproduction number are available in this case, limiting flexibility. The non-mechanistic model in `{EpiNow2}` is equivalent to that used in the [`{EpiEstim}`](https://mrc-ide.github.io/EpiEstim/index.html) R package as they both use a renewal equation to estimate $R_t$ from case time series and the generation interval distribution.

### Exact vs approximate sampling methods

The default sampling method, set through `stan_opts()`, performs [MCMC sampling](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) using [`{rstan}`](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html). The MCMC sampling method is accurate but is often slow. The Laplace, pathfinder, and variational inference methods are faster because they are approximate (See, for example, a detailed explanation for [automatic variational inference in Stan](https://arxiv.org/abs/1506.03431)). In `{EpiNow2}`, you can use varational inference with an `{rstan}` or [`{cmdstanr}`](https://mc-stan.org/cmdstanr/) backend but you must install the latter to access its functionalities. Additionally, `{EpiNow2}` supports using the [Laplace](https://mc-stan.org/docs/cmdstan-guide/laplace_sample_config.html) and [Pathfinder](https://mc-stan.org/docs/cmdstan-guide/pathfinder_config.html) approximate samplers through `{cmdstanr}` but these two methods are currently experimental in `{cmdstanr}` and have not been well tested in the wild. The approximate methods may not be as reliable as the default MCMC sampling method and we do not recommend using them in real-world inference.

### Smoothness/granularity of estimates

The random walk method reduces smoothness/granularity of the estimates, compared to the other methods.

## Caveats

The run times measured here use a crude method that compares the start and end times of each simulation. It only measures the time taken for one model run and may not be accurate. For more accurate run time measurements, we recommend using a more sophisticated approach like those provided by packages like [`{bench}`](https://cran.r-project.org/web/packages/bench/index.html) and [`{microbenchmark}`](https://cran.r-project.org/web/packages/microbenchmark/index.html).

Secondly, we used `r getOption("mc.cores", 1L)` cores for the simulations and so using more or fewer cores might change the run time results. We, however, expect the relative rankings to be the same or similar. To speed up the model runs, we recommend checking the number of cores available on your machine using `parallel::detectCores()` and passing a high enough number of cores to `mc.cores` through the `options()` function. See the benchmarking data setup chunk above for an example.

lastly, the `R` trajectory used to generate the data for benchmarking only represents one scenario. This could favour one model type or solver over another.
