---
title: "Model options: speed versus forecast accuracy tradeoffs"
output:
  bookdown::html_vignette2:
    toc: true
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
vignette: >
  %\VignetteIndexEntry{Model options: speed versus forecast accuracy tradeoffs}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  fig.height = 6.5,
  fig.width = 6.5,
  fig.path = "vignettes/speedup_options-"
)
set.seed(9876)
```

```{r packages}
library(EpiNow2)
library(scoringutils)
library(data.table)
library(rstan)
library(cmdstanr)
library(ggplot2)
library(lubridate)
library(scales)
library(posterior)
```

This vignette explores available model options in _EpiNow2_ and how they vary by speed and accuracy of estimates. We will compare the models by run time and accuracy of estimates (quantitatively and qualitatively) against the "true" trajectory of the data.

## The benchmarking data

To compare the model options, we will need a dataset for which we know the "true" values and trajectories. So, we will start by generating the "true" infections and $R_t$ data using _EpiNow2_'s `forecast_infections()` function. 

`forecast_infections()` requires a fitted estimates object from `epinow()` with `output` set to "fit", the trajectory of the reproduction number, `R`, and the number of samples to simulate. So, we will set these up first.

To obtain the `estimates` object, we will run the `epinow()` function using real-world observed data and delay distributions to recover realistic parameter values. We will use the first $100$ observations of the `example_confirmed` data set, the `example_generation_time`, `example_incubation_period`, and `example_reporting_delay` values that come with _EpiNow2_.

Several of the argument values we will use here will be kept the same for all the model runs, so we will set them up here. These include the generation time, incubation period, reporting delay, observation model options, and the forecast horizon.
```{r, shared_inputs}
# Set the number of cores to use
options(mc.cores = 4)

# Generation time
generation_time <- Gamma(
  shape = Normal(1.3, 0.3),
  rate = Normal(0.37, 0.09),
  max = 14
)

# Incubation period
incubation_period <- LogNormal(
  meanlog = Normal(1.6, 0.05),
  sdlog = Normal(0.5, 0.05),
  max = 14
)

# Reporting delay
reporting_delay <- LogNormal(
  meanlog = 0.5,
  sdlog = 0.5,
  max = 10
)

# Combine the incubation period and reporting delay into one delay
delay <- incubation_period + reporting_delay

# Observation model options
obs <- obs_opts(
  scale = list(mean = 0.1, sd = 0.025),
  return_likelihood = TRUE
)

# No forecasting; just estimation
horizon <- 0
```

Now, let's generate the `estimates` object from `epinow()`.
```{r estimates}
estimates <- epinow(
  data = example_confirmed[1:100],
  generation_time = generation_time_opts(example_generation_time),
  delays = delay_opts(example_incubation_period + example_reporting_delay),
  rt = rt_opts(prior = list(mean = 2, sd = 0.1), rw = 14),
  gp = NULL,
  obs = obs,
  horizon = horizon,
  output = "fit"
)
```

For the `R` data, we will set up an arbitrary trajectory and add some Gaussian noise.
```{r R-data}
# Arbitrary reproduction number trajectory
R <- c(
  rep(2, 40), rep(0.5, 10), rep(1, 10), 1 + 0.04 * 1:20, rep(1.4, 5),
  1.4 - 0.02 * 1:20, rep(1.4, 10), rep(0.8, 5), 0.8 + 0.02 * 1:20
)
# Add Gaussian noise
R_noisy <- R * rnorm(length(R), 1, 0.05)
```

Now, we are ready to simulate the true infections and $R_t$ data by sampling from $10$ posterior samples.
```{r true-data}
# Forecast infections and the trajectory of Rt
forecast <- forecast_infections(
  estimates$estimates,
  R = R_noisy,
  samples = 10
)
```

We will now extract the required data:
- `R_true`: the median of the simulated $R_t$ values,
- `infections_true`: the infections by date of infection, and
- `reported_cases_true`: the reported cases by date of report.
```{r extract-true-data}
R_true <- forecast$summarised[variable == "R"]$median

# Get the posterior samples from which to extract the simulated infections and reported cases
posterior_sample <- forecast$samples[sample == 1]

# Extract the simulated infections
infections_true <- posterior_sample[variable == "infections"]$value

# Extract the simulated reported cases and rename the "value" column to "confirm" (to match EpiNow2 requirements)
reported_cases_true <- posterior_sample[
  variable == "reported_cases", .(date, confirm = value)
]
```

Now, to the main part of this vignette: we will define and run the different model options and evaluate their runtimes and estimation performance.

## Model options

```{r model-descriptions,echo = FALSE}
model_descriptions <- data.table(
  model = c(
    "non_mechanistic",
    "gp",
    "gp_stationary",
    "rw1_no_gp",
    "rw1_gp",
    "rw7_no_gp",
    "rw7_gp",
    "vb_rstan",
    "vb_cmdstanr",
    "pathfinder",
    "laplace"
  ),
  description = c(
    "The model with no priors on Rt",
    "Gaussian process turned on and with a non-stationary prior on $R_t$ (default)",
    "Gaussian process turned on and with a stationary Gaussian process prior on $R_t$",
    "1-day random walk, Gaussian process turned off, and a non-stationary prior on $R_t$ (default)",
    "1-day random walk, Gaussian process turned on, and a non-stationary prior on $R_t$ (default)",
    "7-day random walk, Gaussian process turned off, and a non-stationary prior on $R_t$ (default)",
    "7-day random walk, Gaussian process turned on, and a non-stationary prior on $R_t$ (default)",
    "Variational inference method instead of MCMC for sampling using [`{rstan}`]",
    "Variational inference method instead of MCMC for sampling using [`{cmdstanr}`]",
    "Approximate sampling with the \"pathfinder\" algorithm (from the [`{cmdstanr}`](https://github.com/stan-dev/cmdstanr) package)",
    "Approximate sampling with the \"Laplace\" algorithm (from the [`{cmdstanr}`](https://github.com/stan-dev/cmdstanr) package) for sampling."
  )
)
  
knitr::kable(model_descriptions, caption = "Model options")
```

```{r model-configs, results = 'hide'}
model_configs <- list(
  # The non-mechanistic model
  non_mechanistic = list(rt = NULL),
  # The model with the Gaussian process turned on and a non-stationary prior on R_t (default)
  gp = list(
    rt = rt_opts(
      prior = list(mean = 2, sd = 0.1)
    )
  ),
  # The model with the Gaussian process turned on and the stationary Gaussian process prior on R_t
  gp_stationary = list(
    rt = rt_opts(
      prior = list(mean = 2, sd = 0.1),
      gp_on = "R0"
    )
  ),
  # The model with a 1-day random walk, the Gaussian process turned off and a non-stationary prior on R_t (default)
  rw1_no_gp = list(
    rt = rt_opts(
      prior = list(mean = 2, sd = 0.1),
        rw = 1
      ),
    gp = NULL
  ),
   # The model with a 1-day random walk, the Gaussian process turned on, and a non-stationary prior on R_t (default)
  rw1_gp = list(
    rt = rt_opts(
      prior = list(mean = 2, sd = 0.1),
        rw = 1
      )
  ),
  # The model with a 7-day random walk, the Gaussian process turned off, and a non-stationary prior on R_t (default)
  rw7_no_gp = list(
    rt = rt_opts(
      prior = list(mean = 2, sd = 0.1),
        rw = 7
      ),
    gp = NULL
  ),
  # The model with a 7-day random walk, the Gaussian process turned on, and a non-stationary prior on R_t (default)
  rw7_gp = list(
    rt = rt_opts(
      prior = list(mean = 2, sd = 0.1),
      rw = 7
    )
  ),
  # Model that uses the variational inference method instead of MCMC (from rstan) 
  vb_rstan = list(
    stan = stan_opts(
      method = "vb"
    )
  ),
  # Model that uses the variational inference method instead of MCMC (from cmdstanr) 
  vb_cmdstanr = list(
    stan = stan_opts(
      method = "vb",
      backend = "cmdstanr"
    )
  )#,
  # The model that uses "pathfinder" approximate sampling from `cmdstanr` package
  # pathfinder = list(
  #   stan = stan_opts(
  #     method = "pathfinder",
  #     backend = "cmdstanr",
  #     trials = 5,
  #     samples = 2000
  #   )
  # ),
  # # The model that uses Laplace approximate sampling from `cmdstanr` package
  # laplace = list(
  #   stan = stan_opts(
  #     method = "laplace",
  #     backend = "cmdstanr",
  #     trials = 5,
  #     samples = 2000
  #   )
  # )
)
```

## Running the models

Let's run the models and gather the results.

Let's combine the shared model inputs into a list for use across all the models.
```{r model_inputs}
model_inputs <- list(
  data = reported_cases_true,
  generation_time = generation_time_opts(generation_time),
  delays = delay_opts(delay),
  obs = obs,
  horizon = horizon,
  verbose = FALSE
)
```

To run the models, we will sweep across the list of models `models` and shared model inputs `model_inputs`. We will use the `epinow()` function and return useful outputs like the timing of model runs.
```{r run-models, results = 'hide'}
# Create a version of epinow() that works like base::try() and works even if some models fail.
safe_epinow <- purrr::safely(epinow)
# Run the models
results <- lapply(
  model_configs,
  function(model) {
    do.call(
      safe_epinow,
      c(
        model_inputs,
        model
      )
    )
  }
)
```

## Evaluating performance

### Run times

Let's see how long each model took to run. Note that the run time measured here uses a crude method that compares the start and end times of each simulation. It only measures the time taken for one model run and may not be accurate. For more accurate run time measurements, we recommend using a more sophisticated approach like those provided by packages like [`{bench}`](https://cran.r-project.org/web/packages/bench/index.html) and [`{microbenchmark}`](https://cran.r-project.org/web/packages/microbenchmark/index.html).

Another thing to note is that here, we used `r `getOption("mc.cores", 1L)` cores and so using more or fewer cores might change the runtime results. To speed up the model runs, we recommend checking the number of cores available on your machine using `parallel::detectCores()` and setting a high enough number of cores in `options()`. See the \ref{shared_inputs} chunk above for an example.

```{r runtimes}
# Extract the run times
runtimes <- lapply(
  results,
  function(x) {
    if (is.null(x$error)) {
      return(round(as.duration(x$result$timing), 1))
    } else {
      return(as.duration("NA"))
    }
  }
)

# Convert to a table format

runtimes_dt <- melt(
  as.data.table(runtimes),
  measure.vars = names(model_configs),
  variable.name = "model",
  value.name = "runtime"
)

# Add model descriptions

runtimes_dt <- merge(
  runtimes_dt,
  model_descriptions,
  by = "model"
)

setcolorder(runtimes_dt, c("model", "description", "runtime"))

# Order by run time

runtimes_dt <- runtimes_dt[order(runtime), ]
 
# Print table
knitr::kable(runtimes_dt, caption = "Run times for various _EpiNow2_ models")
```

### Estimation performance

Now, we will compare the estimated and true values using the continuous ranked probability score (CRPS). The CRPS is a proper scoring rule that measures the accuracy of probabilistic forecasts. We will use the `crps()` function from the `{scoringutils}` package.

To calculate the CRPS for the estimated $R_t$ and infections, we will first set up a function that makes sure the true data and estimates are of the same length and calls the `crps_sample()` function from the `{scoringutils}` package.
```{r crps-func}
# A function to calculate the CRPS
calc_crps <- function(x, truth) {
  shortest_obs_length <- min(ncol(x), length(truth))
  reduced_truth <- tail(truth, shortest_obs_length)
  reduced_x <- tail(t(x), shortest_obs_length)
  return(crps_sample(reduced_truth, reduced_x))
}
```

Now, we will extract the $R_t$ and infection estimates and calculate the CRPS using the `calc_crps()` function above.

```{r fit_crps}
# Function to extract Rt estimates
Rt_estimated <- lapply(results, function(x) {
  if (is.null(x$error)) {
    obj <- x$result$estimates$fit
    if (inherits(obj, "stanfit")) {
      if ("R[1]" %in% names(obj)) {
        extract(obj, "R")$R
      } else {
        extract(obj, "gen_R")$gen_R
      }
    } else {
      obj |>
        as_draws_matrix() |> 
        subset_draws(variable = "R")
    }
  } else {
    NA
  }
})
# CRPS for the Rt estimates
rt_crps <- lapply(
  Rt_estimated,
  calc_crps,
  truth = R
)

# Function to extract infection estimates
infections_estimated <- lapply(results, function(x) {
  if (is.null(x$error)) {
  obj <- x$result$estimates$fit
  if (inherits(obj, "stanfit")) {
  extract(obj, "infections")$infections
  } else {
    obj |>
      as_draws_matrix() |> 
      subset_draws(variable = "infections")
    }
  } else {
    NA
  }
})

# CRPS for the infections estimates
infections_crps <- lapply(
  infections_estimated,
  calc_crps,
  truth = infections_true
)
```

We will now post-process the CRPS results to make them easier to visualise by adding a  "time" column and reshaping the data to long format.

```{r fit_postprocessing}
# Post-processing the CRPS results of the Rt estimates
rt_df <- as.data.table(rt_crps)
rt_df[, time := 1:.N]
rt_df <- melt(
  rt_df,
  id.vars = c("time"),
  variable.name = "model"
)

# Post-processing the CRPS results of the infection estimates
infections_df <- as.data.table(infections_crps)
infections_df[, time := 1:.N]
infections_df <- melt(
  infections_df,
  id.vars = c("time"),
  variable.name = "model"
)
```

Let's visualise the CRPS results for the $R_t$ and infection estimates.

```{r rt_plot}
rt_plot <- ggplot(rt_df, aes(x = time, y = value, colour = model)) +
  geom_line() +
  scale_colour_brewer("Model", palette = "Dark2") +
    scale_y_continuous(labels = label_number_auto()) +
  labs(x = "Time", y = "CRPS", title = "Estimating Rt with various model options") +
  ggplot2::theme_bw()
plot(rt_plot)
```

```{r infections_plot}
infections_plot <- ggplot(infections_df, aes(x = time, y = value, colour = model)) +
  geom_line() +
  scale_colour_brewer("Model", palette = "Dark2") +
  scale_y_continuous(labels = label_comma()) +
  labs(x = "Time", y = "CRPS", title = "Estimating infections with various model options") +
  ggplot2::theme_bw()
plot(infections_plot)
```

## Some considerations when using the model options

### Mechanistic vs non-mechanistic models

- Estimation in _EpiNow2_ using the mechanistic approaches (prior on $R_t$) is often much slower than the non-mechanistic approach. The mechanistic model is slower because it models intricate details about the processes and mechanisms that drive $R_t$ estimates using the renewal equation. The non-mechanistic model, on the other hand, runs much faster but does not use the renewal equation to generate infections. Because of this none of the options defining the behaviour of the reproduction number are available in this case, limiting flexibility. It also means that the model is questionable for forecasting. If performing a retrospective analysis, the _non-mechanistic model_ (where `rt = NULL`) is better suited and a faster alternative to the mechanistic $R_t$ model.

### Exact vs approximate methods

- The default method is `method = "sampling"`, which performs MCMC sampling. The MCMC sampling method is accurate but can often be slow. The Laplace, Pathfinder, and variational inference methods are the fastest because they are approximate methods. The first two require having the `cmdstanr` package installed. Also note that the methods are experimental and may not be as reliable as the default MCMC sampling method. For details on the Laplace and Pathfinder approximation methods, see respectively, [here](https://mc-stan.org/docs/cmdstan-guide/laplace_sample_config.html) and [here](https://mc-stan.org/docs/cmdstan-guide/pathfinder_config.html).

### Granularity of estimates

- The random walk method reduces granularity in estimates, compared to the other methods.